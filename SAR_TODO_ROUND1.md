# ðŸ SAR Narrative Generator â€” Round 1 TODO

> **Goal:** Submit Abstract + Semi-Working Prototype with Screenshots/Video  
> **Deadline:** Feb 17, 2026 â€” 11:59 PM IST  
> **Legend:** `[ ]` Todo Â· `[/]` In Progress Â· `[x]` Done

---

## ðŸ‘¥ Team Roles

| Name | Role | Focus Areas |
|------|------|-------------|
| **Shubh** | Backend + AI Integration Lead | FastAPI, API routes, system integration, AI pipeline wiring |
| **Dev** | LLM/RAG + AI Core Lead | Ollama, LangChain, RAG pipeline, audit trail, prompt engineering |
| **Siddh** | Frontend Lead | Streamlit UI, dashboard, SAR editor, visualizations |
| **Het** | Data & ML Engineer | Datasets, data parsing, XGBoost classifier, SHAP |
| **Sakshi** | Docs, PPT & Submission Lead | Submission doc, PPT, screenshots, README |

---

## ðŸ“‚ File Ownership Map (Zero Conflict Guarantee)

Each person works in **completely separate directories/files**. No merge conflicts.

```
Hack-O-Hire/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py                â† SHUBH ONLY
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â”œâ”€â”€ routes.py          â† SHUBH ONLY
â”‚   â”‚   â”‚   â””â”€â”€ schemas.py         â† SHUBH ONLY
â”‚   â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”‚   â”œâ”€â”€ rag_pipeline.py    â† DEV ONLY
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_engine.py      â† DEV ONLY
â”‚   â”‚   â”‚   â””â”€â”€ audit_logger.py    â† DEV ONLY
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”‚       â””â”€â”€ data_parser.py     â† HET ONLY
â”‚   â”œâ”€â”€ knowledge_base/            â† DEV ONLY
â”‚   â””â”€â”€ requirements.txt           â† SHUBH ONLY (others tell Shubh what to add)
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ app.py                     â† SIDDH ONLY
â”‚   â”œâ”€â”€ pages/                     â† SIDDH ONLY
â”‚   â””â”€â”€ components/                â† SIDDH ONLY
â”œâ”€â”€ ml_models/
â”‚   â””â”€â”€ typology_classifier.py     â† HET ONLY
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample_data/               â† HET ONLY
â”œâ”€â”€ screenshots/                   â† SAKSHI ONLY
â”œâ”€â”€ TeamName_CampusName_160226.md  â† SAKSHI ONLY
â”œâ”€â”€ SAR_PPT_DATA.md                â† SAKSHI ONLY
â”œâ”€â”€ docker-compose.yml             â† SHUBH ONLY
â””â”€â”€ README.md                      â† SAKSHI ONLY
```

---

## Phase 1: Setup & Skeleton âœ… (DONE â€” already pushed to GitHub)

> All skeleton files created and pushed. Everyone pulls `main` and runs setup.

```bash
git pull origin main
chmod +x setup.sh
./setup.sh shubh    # Replace "shubh" with your name
# See HOW_TO_RUN.md for full instructions
```

---

## Phase 2: Core Working Prototype (4 Hours â€” All Parallel)

> **Contract:** Everyone produces/consumes data as **JSON dicts** â€” no cross-file imports.  
> **Sync Point:** Quick call after Phase 2 to test connections.

### SHUBH â€” Backend + AI Integration
- [x] Implement `POST /api/upload` â€” parse uploaded CSV using Het's parser, store in memory
- [x] Implement `POST /api/generate-sar` â€” call Dev's `generate_sar()`, return result
- [x] Implement `GET /api/sar/{id}` â€” return stored SAR + audit trail
- [x] Implement `PUT /api/sar/{id}` â€” accept edited narrative, track diff
- [x] Implement `POST /api/sar/{id}/approve` â€” change status to approved
- [x] Wire up Het's `predict_typology()` into the generate pipeline
- [x] Test all endpoints with `curl` / Postman
- [x] Help Dev with any LLM integration issues

### DEV â€” LLM/RAG Core (AI Heavy)
- [x] Install Ollama, pull `llama3.1:8b` model (Download at ~80%)
- [x] Build full SAR generation system prompt (FinCEN format, 5Ws+How, guardrails)
- [x] Index knowledge base docs (SAR templates + FinCEN guidance) into ChromaDB
- [x] Build RAG chain: query â†’ retrieve templates â†’ augment prompt â†’ generate
- [x] Implement LangChain callback handler for audit trail capture
- [x] Create `generate_sar(alert_data: dict) -> dict` returning:
  - `narrative` (intro + body + conclusion)
  - `audit_trail` (list of reasoning steps)
  - `quality_score` (completeness, compliance, readability, evidence)
- [x] Test: Feed `scenario_smurfing.csv` â†’ get a real SAR narrative out
<<<<<<< Updated upstream
- [x] Experiment with prompt quality â€” iterate until narrative reads professional
=======
- [/] Experiment with prompt quality â€” iterate until narrative reads professional
>>>>>>> Stashed changes

### SIDDH â€” Frontend (Streamlit UI)
- [x] **Dashboard:** Case list with status badges (Draft/Review/Approved), metrics cards âœ…
- [x] **Upload page:** File uploader â†’ show parsed data in table âœ…
- [x] **SAR Editor:** Split-screen (data left, narrative editor right), Approve/Reject/Export buttons âœ…
- [x] **Audit Trail:** Expandable cards, color-coded steps âœ…
- [x] **Wire up to backend API (`requests.get/post` to FastAPI) â€” `api_client.py` + all pages wired âœ…
- [x] **IMPORTANT:** If backend not ready, use **mock JSON** so UI is fully demo-able âœ…
- [x] **Add loading spinners, dark mode polish âœ…

### HET â€” Data & ML
- [x] Download SAML-D dataset (or subset ~50K rows) from Kaggle
- [x] Create `data_parser.py` â€” CSV/JSON â†’ normalized dict format
- [x] Create 2 more demo CSVs: `scenario_layering.csv`, `scenario_structuring.csv`
- [x] Train XGBoost on SAML-D features (amount, frequency, counterparties, time gaps)
- [x] Save model as `.joblib`
- [x] Implement `predict_typology(transactions) -> dict` with confidence + feature importance
- [x] Add SHAP explanations for top features

### SAKSHI â€” Docs & PPT
- [ ] Finalize submission doc â€” replace [TeamName], [CampusName], member names
- [ ] Polish Abstract (100-200 words, crisp)
- [ ] Start building PPT (use `SAR_PPT_DATA.md` content)
- [ ] Take screenshots of Siddh's UI (mock data is fine for now) â€” **UI is demo-ready now!**
- [ ] Research competitor SAR tools for "why we're better" slide

---

## Phase 3: Integration & Demo-Ready (3 Hours)

> **Shubh leads integration.** Others provide their modules.

### SHUBH â€” Integration Captain
- [x] Integrate Dev's `generate_sar()` into API route
- [x] Integrate Het's `predict_typology()` into pipeline
- [x] Test full E2E: Upload â†’ Generate â†’ Audit â†’ Edit â†’ Approve
- [x] Fix any integration bugs
- [ ] Help record demo video

### DEV â€” LLM Quality + Polish
- [x] Improve prompt based on initial outputs
- [x] Add quality scoring logic (are all 5Ws present?)
- [x] Test all 3 demo scenarios â€” ensure good output
- [x] Help Shubh with integration debugging

### SIDDH â€” Frontend Final + Video
- [x] Connect to live backend (replace mocks) â€” graceful fallback when offline âœ…
- [x] Polish UI (animations, error handling) â€” fade-in, pulse, hover, toast âœ…
- [ ] Take **final screenshots** of working prototype
- [ ] **Record demo video** (3-4 min screen recording)

### HET â€” Final Model + Data
- [x] Verify classifier accuracy (aim >80%) â€” achieved 100%
- [x] Generate predictions for all 3 demo scenarios
- [x] Create accuracy/F1/precision stats for submission doc
- [x] Ensure data parser handles all edge cases

### SAKSHI â€” Final Submission
- [ ] Update submission doc with final screenshots + metrics
- [ ] Add sample SAR narrative output to document
- [ ] Final proofread
- [ ] Convert to `.doc` format
- [ ] Build final PPT
- [ ] Rename: `TeamName_CampusName_170226.doc` / `.ppt`
- [ ] Verify size < 45 MB
- [ ] **SUBMIT** ðŸŽ‰

---

## â° Timeline

| Time | Phase | Lead |
|------|-------|------|
| **12:30 AM â€“ 4:30 AM** (Feb 17) | Phase 2: Core Prototype | All parallel |
| 4:30 AM | âš¡ **Sync call** (10 min) | Test connections |
| **4:30 AM â€“ 7:30 AM** | Phase 3: Integration + Polish | Shubh leads |
| **8:00 AM â€“ 12:00 PM** | Final polish + Submission prep | Sakshi leads |
| **12:00 PM â€“ 11:59 PM** | Buffer + Submit | |

---

## ðŸ¤ Integration Contracts

### Het â†’ Shubh (Data Parser â†’ API)
```json
{
  "case_id": "CASE-001",
  "transactions": [
    { "txn_id": "TXN-001", "sender": "Account-A", "receiver": "Account-B",
      "amount": 500000, "currency": "INR", "timestamp": "2026-01-15T10:30:00", "type": "NEFT" }
  ],
  "customer": {
    "name": "Rajesh Kumar", "account_id": "XXXX-4521",
    "kyc_status": "verified", "business_type": "Textile Export", "avg_monthly_volume": 300000
  }
}
```

### Shubh â†’ Dev (API â†’ LLM Engine)
```json
{
  "sar_id": "SAR-001",
  "narrative": { "introduction": "...", "body": "...", "conclusion": "..." },
  "audit_trail": [
    { "step": 1, "agent": "data_analyst", "action": "...", "data_points_used": [...], "output": "..." }
  ],
  "quality_score": { "completeness": 0.95, "compliance": 0.98, "readability": 0.88, "evidence_linkage": 0.91 },
  "typology": { "prediction": "structuring", "confidence": 0.92 },
  "status": "draft"
}
```

### Het â†’ Dev (ML Model â†’ LLM Engine)
```json
{
  "typology": "structuring",
  "confidence": 0.92,
  "top_features": { "num_unique_senders": 0.35, "total_amount": 0.28, "time_window_days": 0.22 }
}
```

---

## ðŸŽ¯ Round 1 Submission Checklist

- [x] Project skeleton pushed to GitHub âœ“
- [ ] Abstract document (100-200 words)
- [ ] System Architecture + component descriptions
- [ ] Methodology (scalability, performance, security)
- [ ] Tech stack listing
- [ ] Future scope
- [ ] Screenshots of working prototype (min 3)
- [ ] Demo video (3-4 min)
- [x] Semi-working prototype (Streamlit UI with mock data) âœ“
- [ ] File: `TeamName_CampusName_170226.doc` + `.ppt`
- [ ] Total size < 45 MB

---

## ðŸ“„ Reference Files

- **`HOW_TO_RUN.md`** â€” Step-by-step setup & run instructions
- **`Downloaded_Things.md`** â€” What's installed, sizes, who needs what
- **`setup.sh`** â€” One-command setup (run: `./setup.sh <your-name>`)
- **`docker-compose.yml`** â€” Ollama + PostgreSQL via Docker
